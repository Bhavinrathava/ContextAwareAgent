import os 
import pinecone
import re
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains.conversation.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain

class ChatClass:
    def __init__(self, openAIKey, pineConeKey) -> None:
        self.initiatedModel = True
        self.userQuestion = None
        self.previousQuestions = []
        self.conversation_sum_bufw = None
        self.bufw_history = None
        self.OpenAIkey = openAIKey
        self.pineConeKey = pineConeKey
        self.initiateModel()

    def generateBaseQuestion(self, userQuestion):
        basePrompt = "You are a advanced Document Q and A agent that answers user queries \
        based on imformation provided in a speicific document. CUrrently the user has \
        asked a question, possibly, this is not the first question that the user has asked to you. \
        You will be provided a chat history between the user and responses generated by system along \
        the current question posted by user. You need to combine the chat history and the current question \
        and generate a single question only and nothing else which can be used to find the answer in the document. \n \n "

        if(len(self.bufw_history) == 0):
            self.bufw_history = "No History Available "

        chatHistoryPrompt = f"Chat History : \n \n {self.bufw_history}"

        userQuestionPrompt = f"User Question : \n \n {userQuestion}"

        prompt = basePrompt + chatHistoryPrompt + userQuestionPrompt

        # Generate the answer
        generatedQuestion = self.conversation_sum_bufw(prompt)

        return generatedQuestion['response']
    
    def sanitizeFileName(self, filename):
        # Convert the filename to lowercase
        filename_lower = filename.lower()
        
        # Use regular expression to remove non-alphanumeric characters
        # This regex will keep lowercase letters, numbers, and the dot character (for file extension)
        sanitized_filename = re.sub(r'[^a-z0-9\.]', '', filename_lower)
        return sanitized_filename

    def findRelevantSources(self, generatedQuestion, documentNameSpace = "contextdatascience"):
        # Convert the generated question to embeddings
        embeddingModel = OpenAIEmbeddings(openai_api_key = self.OpenAIkey)

        # Convert the data to embeddings - we expect a list of strings as input (Data)
        embeddings = embeddingModel.embed_documents([generatedQuestion])
        embeddings = embeddings[0]
        sanitized_filename = self.sanitizeFileName(documentNameSpace)
        pinecone.init(api_key=self.pineConeKey, environment='gcp-starter')

        index = pinecone.Index("contextdatascience")

        relevantSources = index.query(vector=embeddings, top_k=5,include_metadata=True, namespace=sanitized_filename)

        #print(relevantSources)
        sources = ""
        for source in relevantSources['matches']:
            sources = sources + source['metadata']['values'] + "\n \n"

        return sources


    def generateAnswer(self, sources, generatedQuestion):
        secondBasePrompt = "You will be asked a question and will be provided relevant document snippets. \
    I want you to read the document snippets and answer the question based on only the document snippets provided. \n \n"

        documentPrompt = f"Document Snippets : \n \n {sources}"
        questionPrompt = f"Question : \n \n {generatedQuestion}"

        secondPrompt = secondBasePrompt + documentPrompt + questionPrompt

        #print(secondPrompt)

        # Ask the model to generate the answer
        generatedAnswer = self.conversation_sum_bufw(secondPrompt)

        return (generatedAnswer['response'])
    

    def initiateModel(self):
            try:
                model = ChatOpenAI(openai_api_key = self.OpenAIkey, model_name="gpt-4")
            except:
                print("There was an error in initiating the model. Please check the OpenAI Key!")

            # Ask the model to generate the answer
            conversation_sum_bufw = ConversationChain(llm=model, memory=ConversationSummaryBufferMemory(llm=model,max_token_limit=650))
            bufw_history = conversation_sum_bufw.memory.load_memory_variables(inputs=[])['history']

            self.conversation_sum_bufw= conversation_sum_bufw
            self.bufw_history = bufw_history

    def getAnswer(self, textInput, documentNameSpace = "contextdatascience"):
        baseQuestion = self.generateBaseQuestion(textInput)
        relevantSources = self.findRelevantSources(baseQuestion, documentNameSpace)
        answer = self.generateAnswer(relevantSources, baseQuestion)
        return answer


